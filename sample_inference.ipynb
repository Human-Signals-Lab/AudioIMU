{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from ParticipantLab import ParticipantLab as parti\n",
    "from copied_from_guihong_server.models import DeepConvLSTM_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of participants: 15 15\n"
     ]
    }
   ],
   "source": [
    "P = 15\n",
    "win_size = 10\n",
    "hop = .5\n",
    "\n",
    "participants = []\n",
    "\n",
    "# prepare user data\n",
    "PATH_data = '/media/hd4t1/dawei/audioimu/lab'\n",
    "\n",
    "if os.path.exists(PATH_data + '/rawAudioSegmentedData_window_' + str(win_size) + '_hop_' + str(hop) + '_Test_NEW.pkl'):\n",
    "    with open(PATH_data + '/rawAudioSegmentedData_window_' + str(win_size) + '_hop_' + str(hop) + '_Test_NEW.pkl', 'rb') as f:\n",
    "        participants = pickle.load(f)\n",
    "\n",
    "\n",
    "# load user data\n",
    "window_size = 1024\n",
    "hop_size = 320\n",
    "batch_size = 64  \n",
    "model_name =  'DeepConvLSTM_Split' \n",
    "fmin, fmax = 50, 11000\n",
    "mel_bins = 64\n",
    "classes_num = 23\n",
    "sr = 22050\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 200\n",
    "device = 'cuda'\n",
    "sub_list = np.arange(15)\n",
    "\n",
    "print(\"# of participants:\", len(sub_list), len(participants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test participant (lopo): 01\n",
      "GPU number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1850: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST f1: 0.6659 reported f1: 0.637\n",
      "TEST acc: 0.7115 reported acc: 0.7115\n",
      "test participant (lopo): 02\n",
      "GPU number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST f1: 0.649 reported f1: 0.649\n",
      "TEST acc: 0.6638 reported acc: 0.6638\n",
      "test participant (lopo): 03\n",
      "GPU number: 2\n",
      "TEST f1: 0.625 reported f1: 0.625\n",
      "TEST acc: 0.6445 reported acc: 0.6445\n",
      "test participant (lopo): 04\n",
      "GPU number: 2\n",
      "TEST f1: 0.7544 reported f1: 0.7544\n",
      "TEST acc: 0.7813 reported acc: 0.7813\n",
      "test participant (lopo): 05\n",
      "GPU number: 2\n",
      "TEST f1: 0.7333 reported f1: 0.7333\n",
      "TEST acc: 0.7492 reported acc: 0.7492\n",
      "test participant (lopo): 06\n",
      "GPU number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST f1: 0.6286 reported f1: 0.6286\n",
      "TEST acc: 0.6716 reported acc: 0.6716\n",
      "test participant (lopo): 07\n",
      "GPU number: 2\n",
      "TEST f1: 0.73 reported f1: 0.73\n",
      "TEST acc: 0.7383 reported acc: 0.7383\n",
      "test participant (lopo): 08\n",
      "GPU number: 2\n",
      "TEST f1: 0.8681 reported f1: 0.8681\n",
      "TEST acc: 0.8776 reported acc: 0.8776\n",
      "test participant (lopo): 09\n",
      "GPU number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST f1: 0.618 reported f1: 0.618\n",
      "TEST acc: 0.6545 reported acc: 0.6545\n",
      "test participant (lopo): 10\n",
      "GPU number: 2\n",
      "TEST f1: 0.7962 reported f1: 0.7962\n",
      "TEST acc: 0.805 reported acc: 0.805\n",
      "test participant (lopo): 11\n",
      "GPU number: 2\n",
      "TEST f1: 0.7955 reported f1: 0.7955\n",
      "TEST acc: 0.8052 reported acc: 0.8052\n",
      "test participant (lopo): 12\n",
      "GPU number: 2\n",
      "TEST f1: 0.7257 reported f1: 0.7257\n",
      "TEST acc: 0.7548 reported acc: 0.7548\n",
      "test participant (lopo): 13\n",
      "GPU number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST f1: 0.7511 reported f1: 0.7511\n",
      "TEST acc: 0.7816 reported acc: 0.7816\n",
      "test participant (lopo): 14\n",
      "GPU number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST f1: 0.7117 reported f1: 0.7117\n",
      "TEST acc: 0.7429 reported acc: 0.7429\n",
      "test participant (lopo): 15\n",
      "GPU number: 2\n",
      "TEST f1: 0.6872 reported f1: 0.6872\n",
      "TEST acc: 0.7115 reported acc: 0.7115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawei/anaconda3/envs/tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "overall_acc, overall_f1 = [], []\n",
    "test_oo_overall, true_test_oo_overall = [], []\n",
    "\n",
    "for sub in sub_list:\n",
    "    # load training set\n",
    "    X_trainM = np.empty((0,np.shape(participants[0].rawMdataX_s1)[1], np.shape(participants[0].rawMdataX_s1)[-1]))\n",
    "    X_trainA = np.empty((0,np.shape(participants[0].rawAdataX_s1)[-1]))\n",
    "    y_train = np.zeros((0, 1))\n",
    "        \n",
    "    # load val set\n",
    "    X_testM = np.empty((0,np.shape(participants[0].rawMdataX_s1)[1], np.shape(participants[0].rawMdataX_s1)[-1]))\n",
    "    X_testA = np.empty((0,np.shape(participants[0].rawAdataX_s1)[-1]))\n",
    "    y_test = np.zeros((0, 1))\n",
    "        \n",
    "    for u in [participants[sub]]:\n",
    "        print(\"test participant (lopo): \" + u.name)\n",
    "        #print(X_testM.shape)\n",
    "        X_testM = np.vstack((X_testM, u.rawMdataX_s1[:]))\n",
    "        X_testM = np.vstack((X_testM, u.rawMdataX_s2[:]))\n",
    "        X_testA = np.vstack((X_testA, u.rawAdataX_s1[:]))\n",
    "        X_testA = np.vstack((X_testA, u.rawAdataX_s2[:]))\n",
    "        y_test = np.vstack((y_test, u.rawdataY_s1))\n",
    "        y_test = np.vstack((y_test, u.rawdataY_s2))\n",
    "        #print(X_testM.shape, X_testA.shape, y_test.shape)\n",
    "        #y_test = np.vstack((u.rawdataY_s1, u.rawdataY_s2))\n",
    "    \n",
    "    # filter out NULL\n",
    "    y_test = y_test.flatten()\n",
    "    X_testM = X_testM[y_test != 23]\n",
    "    X_testA = X_testA[y_test != 23]\n",
    "    y_test = y_test[y_test != 23]\t\n",
    "    \n",
    "    y_train = y_train.flatten()\n",
    "    X_trainM = X_trainM[y_train != 23]\n",
    "    X_trainA = X_trainA[y_train != 23]\t\n",
    "    y_train = y_train[y_train != 23]\n",
    "    \n",
    "    # one hot\n",
    "    y_test = y_test.astype('int64')\n",
    "    y_train = y_train.astype('int64')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    Model = eval(model_name)\n",
    "    \n",
    "    # CNN\n",
    "    model = Model(classes_num=classes_num, acc_features=np.shape(X_trainM[:,:,:3])[-1], gyr_features = np.shape(X_trainM[:,:,3:])[-1])\n",
    "    \n",
    "    # Parallel\n",
    "    print('GPU number: {}'.format(torch.cuda.device_count()))\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    if 'cuda' in str(device):\n",
    "        model.to(device)\n",
    "    \n",
    "    pretrained_checkpoint_path = './copied_from_guihong_server/IMU_models/DeepConvLSTM_Split_NEW_kd_expr2 (teacher2)/participant_%s' % u.name\n",
    "    checkpoint_model = [os.path.join(pretrained_checkpoint_path, i) for i in os.listdir(pretrained_checkpoint_path) if i.endswith('.pth')][0]\n",
    "    reported_acc, reported_f1 = checkpoint_model.split('=')[1][:6], checkpoint_model.split('=')[2][:6]\n",
    "    reported_acc, reported_f1 = float(reported_acc), float(reported_f1)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_model, map_location='cuda')\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    \n",
    "    x_trainM_tensor = torch.from_numpy(np.array(X_trainM)).float()\n",
    "    x_trainA_tensor = torch.from_numpy(np.array(X_trainA)).float()\n",
    "    y_train_tensor = torch.from_numpy(np.array(y_train)).float()\n",
    "    x_testM_tensor = torch.from_numpy(np.array(X_testM)).float()\n",
    "    x_testA_tensor = torch.from_numpy(np.array(X_testA)).float()\n",
    "    y_test_tensor = torch.from_numpy(np.array(y_test)).float()\n",
    "    \n",
    "    test_data = TensorDataset(x_testM_tensor, x_testA_tensor, y_test_tensor)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=8, pin_memory=True, shuffle = False)\n",
    "    \n",
    "    eval_output = []\n",
    "    true_output = []\n",
    "    test_output = []\n",
    "    true_test_output = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_val, x_val_a, y_val in test_loader:\n",
    "            # split acc and gyro inputs for as required by the student\n",
    "            acc_val, gyr_val =  np.expand_dims(np.array(x_val[:,:,:3]), axis = 1), np.expand_dims(np.array(x_val[:,:,3:]), axis = 1)               \n",
    "            acc_val = torch.from_numpy(np.array(acc_val)).float()\n",
    "            acc_val = acc_val.to(device)\n",
    "            gyr_val = torch.from_numpy(np.array(gyr_val)).float()\n",
    "            gyr_val = gyr_val.to(device)\n",
    "            x_val_a = x_val_a.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            yhat = model(acc_val, gyr_val)['clipwise_output']\n",
    "    \n",
    "            test_output.append(yhat.data.cpu().numpy())\n",
    "            true_test_output.extend(y_val.data.cpu().numpy())\n",
    "\n",
    "        test_oo = np.argmax(np.vstack(test_output), axis = 1)\n",
    "        true_test_oo = np.asarray(true_test_output)\n",
    "\n",
    "        accuracy = round(metrics.balanced_accuracy_score(true_test_oo, test_oo), 4)\n",
    "\n",
    "        precision, recall, fscore,_ = metrics.precision_recall_fscore_support(true_test_oo, test_oo, labels=np.unique(true_test_oo), average='macro')\n",
    "        fscore = round(fscore, 4)\n",
    "        \n",
    "    print('TEST f1: {}'.format(fscore), 'reported f1:', reported_f1)\n",
    "    print('TEST acc: {}'.format(accuracy), 'reported acc:', reported_acc)\n",
    "    overall_acc.append(accuracy)\n",
    "    overall_f1.append(fscore)\n",
    "    test_oo_overall.extend(test_oo)\n",
    "    true_test_oo_overall.extend(true_test_oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall acc, f1: 0.7395533333333333 0.7159800000000001\n"
     ]
    }
   ],
   "source": [
    "print('overall acc, f1:', np.mean(overall_acc), np.mean(overall_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
